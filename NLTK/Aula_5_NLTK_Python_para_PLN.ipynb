{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula_5_NLTK_Python_para_PLN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guimol/NLP-Projects/blob/main/NLTK/Aula_5_NLTK_Python_para_PLN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOA0LNAjL2bS"
      },
      "source": [
        "# Importando o NLTK\n",
        "\n",
        "Importar um módulo ou biblioteca significa informar para o programa que você está criando/executando que precisa daquela biblioteca específica.\n",
        "\n",
        "É possível fazer uma analogia, imagine que você precisa estudar para as provas de Matemática e Português. Você pega seus livros para estudar. Nessa analogia os livros são as \"bibliotecas externas\" nas quais você quer estudar o assunto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAhFdxNexiye"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fofl3BUKL6uc"
      },
      "source": [
        "# Fazendo o download dos dados complementares do NLTK\n",
        "\n",
        "Os desenvolvedores do NLTK decidiram manter o arquivo de instalação (pip install nltk) com o mínimo de arquivos possível para facilitar o download e instalação. Portanto, eles permitem fazer o download dos arquivos complementares de acordo com a demanda dos desenvolvedores. \n",
        "\n",
        "Para fazer isso, basta executar o código abaixo e seguir as instruções apresentadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjYDsdnhyIK7"
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMrXAgo9MBWH"
      },
      "source": [
        "# O que encontramos no NLTK?\n",
        "\n",
        "As células abaixo apresentam o exemplo de um dos córpus em Português que podemos acessar com o NLTK. \n",
        "\n",
        "MACMORPHO - http://nilc.icmc.usp.br/macmorpho/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZEiXffDz3Do"
      },
      "source": [
        "# Mostrar as palavras existentes no MACMorpho\n",
        "# Observe que elas estão dispostas em uma estrutura de Lista\n",
        "# Observe também a estrutura para acessar o córpus e seus tokens, imagine \n",
        "#   que está acessando uma estrutura de árvore, com uma raiz e vários ramos filhos.\n",
        "\n",
        "nltk.corpus.mac_morpho.words()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvF41ORQ0vKL"
      },
      "source": [
        "nltk.corpus.mac_morpho.sents()[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHMm8CPz1SIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6b985d-593d-40fa-cfe5-906ce832aa7b"
      },
      "source": [
        "nltk.corpus.mac_morpho.tagged_words()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Jersei', 'N'), ('atinge', 'V'), ('média', 'N'), ...]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtCC0wPq1p8Z"
      },
      "source": [
        "nltk.corpus.mac_morpho.tagged_sents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kqU92ykMKlO"
      },
      "source": [
        "# Primeira tarefa com o NLTK - a Tokenização\n",
        "\n",
        "Observe que essa é a forma mais simples de tokenizar um texto usando o NLTK.\n",
        "\n",
        "A função (trecho de código pré-desenvolvido que executa uma ação) *word_tokenize()* recebe um texto e retorna uma lista de tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QotSBAFa-yo2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "outputId": "4c56a8e2-e2ef-4da3-ff40-7cbdc573225a"
      },
      "source": [
        "nltk.word_tokenize(\"Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o New York Giants anotou o touchdown decisivo e derrubou o favorito New England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b954af071105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o New York Giants anotou o touchdown decisivo e derrubou o favorito New England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipyMynSwMOi4"
      },
      "source": [
        "# Formas adicionais avançadas para tokenização de um texto\n",
        "\n",
        "O conceito utilizado nas células seguintes é o de Expressões Regulares. \n",
        "\n",
        "Expressões regulares (chamadas REs, ou regexes ou padrões regex) são essencialmente uma mini linguagem de programação altamente especializada incluída dentro do Python. \n",
        "\n",
        "Usando esta pequena linguagem, você especifica as regras para o conjunto de strings possíveis que você quer combinar; esse conjunto pode conter sentenças em inglês, endereços de e-mail, ou comandos TeX ou qualquer coisa que você queira. Você poderá então perguntar coisas como “Essa string se enquadra dentro do padrão?” ou “Existe alguma parte da string que se enquadra nesse padrão?”. Você também pode usar as REs para modificar uma string ou dividi-la de diversas formas.\n",
        "\n",
        "https://docs.python.org/pt-br/3.8/howto/regex.html\n",
        "\n",
        "https://www.w3schools.com/python/python_regex.asp\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09jsmJtjBFyM"
      },
      "source": [
        "# Informando ao programa que vamos utilizar a classe RegexpTokenizer\n",
        "# observe que é outra forma de fazer a 'importação' de um módulo\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Nosso texto\n",
        "texto = \"Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o New York Giants anotou o touchdown decisivo e derrubou o favorito New England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII.\"\n",
        "\n",
        "# Criando o \"objeto\" que vai tokenizar nosso texto.\n",
        "# Nesse caso usamos uma expressão regular que vai retornar todos os tokens\n",
        "#  textuais (letras do alfabeto, números e underscore). \n",
        "#  Não queremos os símbolos.\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# Executando o método do objeto tokenizador\n",
        "tokens = tokenizer.tokenize(texto)\n",
        "\n",
        "# Nossos tokens :)\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNMY7w9jDUO7"
      },
      "source": [
        "# Informando ao programa que vamos utilizar a classe RegexpTokenizer\n",
        "# observe que é outra forma de fazer a 'importação' de um módulo\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Nosso texto\n",
        "texto = \"Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o New York Giants anotou o touchdown decisivo e derrubou o favorito New England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII.\"\n",
        "\n",
        "# Criando o \"objeto\" que vai tokenizar nosso texto.\n",
        "# Nesse caso usamos uma expressão regular que vai retornar somente os tokens\n",
        "#  com letras maiúsculas e minúsculas. Não queremos os símbolos e números.\n",
        "tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w+')\n",
        "\n",
        "tokens = tokenizer.tokenize(texto)\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FYMuJN3MY2i"
      },
      "source": [
        "# Frequência de tokens\n",
        "\n",
        "Muitas vezes é interessante saber a frequencia em que os tokens aparecem em um texto. Com a classe *FreqDist* podemos calcular facilmente.\n",
        "\n",
        "**Nesse primeiro exemplo, como será a frequencia usando todos os tokens?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvahZUEmFV1g"
      },
      "source": [
        "# Nosso texto\n",
        "texto = \"Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o New York Giants anotou o touchdown decisivo e derrubou o favorito New England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII.\"\n",
        "\n",
        "# Tokenizamos nosso texto usando a word_tokenize\n",
        "tokens = nltk.word_tokenize(texto)\n",
        "\n",
        "# Calculando nossa frequencia de palavras\n",
        "frequencia = nltk.FreqDist(tokens)\n",
        "\n",
        "# Recuperamos a lista de frequencia usando a função most_common()\n",
        "frequencia.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9wUgKFEMeM1"
      },
      "source": [
        "**E se excluírmos as pontuações?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL3l9yQnGWj7"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "texto = \"Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o New York Giants anotou o touchdown decisivo e derrubou o favorito New England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII.\"\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens = tokenizer.tokenize(texto)\n",
        "\n",
        "frequencia = nltk.FreqDist(tokens)\n",
        "frequencia.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l7M6CZnMpve"
      },
      "source": [
        "# Acessando córpus externos\n",
        "\n",
        "Como já foi apresentado, podemos acessar nossos arquivos que estão no Google Drive apenas \"montando\" nosso drive no ícone na barra à esquerda. \n",
        "\n",
        "Para acessar o conteúdo do arquivo, devemos usar a função *open()* que está embutida no python. Essa função retorna o arquivo no formato que o python entende. Para lermos o seu conteúdo devemos usar a função *read()*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYomAwvfHTHF"
      },
      "source": [
        "# Abrindo nosso córpus\n",
        "# Nesse código concatenamos a função open com a função read\n",
        "# Sem concatenar teríamos a seguinte construção\n",
        "#   infile = open('/content/drive/MyDrive/recursos/corpus_teste.txt')\n",
        "#   corpus = infile.read()\n",
        "\n",
        "corpus = open('/content/drive/MyDrive/Codigos/BootcampPythonPLN/nltk/corpus_teste.txt').read()\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGL3NvKOMu9F"
      },
      "source": [
        "**Agora vamos tokenizar e calcular a frequência do nosso corpus inteiro :)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8fB03n6H0oS"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Não quero símbolos\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens = tokenizer.tokenize(corpus)\n",
        "\n",
        "frequencia = nltk.FreqDist(tokens)\n",
        "frequencia.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcl_3GE-M1L0"
      },
      "source": [
        "# Agrupando minúsculas e maiúsculas\n",
        "\n",
        "Nas células anteriores percebemos que alguns tokens estão com o texto em maiúsculas e outros em minúsculas. O python considera que são tokens diferentes apenas por conter letras com \"caixa\" diferente. Portanto, precisamos agrupar todas as palavras que sabemos que são a mesma coisa. O modo mais simples é converter todas para minúsculas ou maiúsculas.\n",
        "\n",
        "Vimos que podemos modificar uma string para minúsculas ou maiúsculas apenas usando as funções *.lower()* ou *.upper()*, respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx-YnEGnIsVb"
      },
      "source": [
        "# Vamos usar o tokenizador do tipo Regex\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Vamos considerar apenas as letras\n",
        "tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w*')\n",
        "\n",
        "# Tokenizamos o corpus\n",
        "tokens = tokenizer.tokenize(corpus)\n",
        "\n",
        "# Nesse trecho queremos criar uma nova lista com todos os tokens convertidos em\n",
        "# minúsculas. Para fazer isso \"caminhamos\" na nossa lista de tokens e executamos\n",
        "# em cada um a função .lower() e adicionamos esse token convertido na nova lista.\n",
        "nova_lista = []\n",
        "\n",
        "for token in tokens:\n",
        "  nova_lista.append(token.lower())\n",
        "\n",
        "# Com todos os tokens convertidos para minúsculas, calcularemos as suas frequencias :)\n",
        "frequencia = nltk.FreqDist(nova_lista)\n",
        "frequencia.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkYotK85M7mi"
      },
      "source": [
        "# Tokens que não nos interessam\n",
        "\n",
        "Alguns tokens que são muito frequentes não ajudam na análise de um texto.\n",
        "Veja como exemplo a lista de tokens anterior, no topo da lista estão artigos, preposições e etc. No nosso caso não são interessantes. \n",
        "\n",
        "O NLTK possui uma lista de tokens considerados desinteressantes e que podem ser removidos de uma lista de tokens sem problemas. Em PLN os chamamos de *stopwords*.\n",
        "\n",
        "Para removê-los da nossa lista de tokens, precisamos comparar um a um com a lista de *stopwords*. Caso um token seja uma *stopword* o removeremos da lista de tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMDOSjJhMPih"
      },
      "source": [
        "# Acessamos a lista de stopwords do NLTK, para a língua portuguesa\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzzoj8scNBeS"
      },
      "source": [
        "# Mais uma vez usarmos o tokenizador de Regex\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Somente as palavras\n",
        "tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w*')\n",
        "tokens = tokenizer.tokenize(corpus)\n",
        "\n",
        "# agora além de convertermos a lista de tokens em minúsculas, vamos comparar\n",
        "# cada token com a lista de stopwords. Somente vamos adicionar à nova lista \n",
        "# os tokens que não forem stopwords\n",
        "nova_lista = []\n",
        "\n",
        "for token in tokens:\n",
        "  if token.lower() not in stopwords:\n",
        "    nova_lista.append(token.lower())\n",
        "\n",
        "# E agora calculamos a frequencia novamente\n",
        "frequencia = nltk.FreqDist(nova_lista)\n",
        "frequencia.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMVXxZovNCuy"
      },
      "source": [
        "#  List Comprehension\n",
        "\n",
        "A técnica de *list comprehension* é uma forma diferente e avançada de criar uma lista. Não é obrigatório saber usá-la, mas é muito interessante conhecer sua construção.\n",
        "\n",
        "O python entende que é uma *list comprehension* quando criamos um laço de repetição entre colchetes: [i for i in range(10)]. Essa construção criará a seguinte lista: [0,1,2,3,4,5,6,7,8,9]. Veja que é possível fazer isso sem essa construção.\n",
        "\n",
        "Uma forma genérica de imaginar uma *list comprehension* é montar a seguinte estrutura: \n",
        "\n",
        "<*lista_final* = **[** *elemento_da_lista* **for** *elemento_da_lista* **in** *lista_de_elementos* **]**>\n",
        "\n",
        "Lembrando que você poderá acrescentar alguma condição para o elemento ser acrescentado na lista:\n",
        "\n",
        "<*lista_final* = **[** *elemento_da_lista* **for** *elemento_da_lista* **in** *lista_de_elementos* **if** *condição* **]**>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVV5GpUJPG4o"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w*')\n",
        "tokens = tokenizer.tokenize(corpus)\n",
        "\n",
        "nova_lista = []\n",
        "\n",
        "#for token in tokens:\n",
        "#  if token.lower() not in stopwords:\n",
        "#    nova_lista.append(token.lower())\n",
        "\n",
        "nova_lista = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
        "\n",
        "frequencia = nltk.FreqDist(nova_lista)\n",
        "frequencia.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYTS2NPKsyuG"
      },
      "source": [
        "# Utilizando ngrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qZrmW4kracZ"
      },
      "source": [
        "# Abrindo nosso córpus\n",
        "# Nesse código concatenamos a função open com a função read\n",
        "# Sem concatenar teríamos a seguinte construção\n",
        "#   infile = open('/content/drive/MyDrive/recursos/corpus_teste.txt')\n",
        "#   corpus = infile.read()\n",
        "\n",
        "corpus = open('/content/drive/MyDrive/Codigos/BootcampPythonPLN/nltk/corpus_teste.txt').read()\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EPWo9outG4u"
      },
      "source": [
        "from nltk import bigrams\n",
        "from nltk import trigrams\n",
        "from nltk import ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMpR47IYtWcc"
      },
      "source": [
        "tokens = nltk.word_tokenize(corpus)\n",
        "\n",
        "tokens_bigrams = list(bigrams(tokens))\n",
        "\n",
        "tokens_bigrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kISKmRVuNsP"
      },
      "source": [
        "tokens_trigrams = list(trigrams(tokens))\n",
        "\n",
        "tokens_trigrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW7aX3DnueT1"
      },
      "source": [
        "tokens_ngrams = list(ngrams(tokens, 4))\n",
        "\n",
        "tokens_ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgGFfYGZvk97"
      },
      "source": [
        "# Reconhecer entidades nomeadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw2i3TgmvkRF"
      },
      "source": [
        "from nltk import bigrams\n",
        "from nltk import trigrams\n",
        "\n",
        "bigramas = list(bigrams(tokens))\n",
        "trigramas = list(trigrams(tokens))\n",
        "\n",
        "for bigrama in bigramas:\n",
        "  if bigrama[0][0].isupper() and bigrama[1][0].isupper():\n",
        "    print(bigrama)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chH65lD2y97X"
      },
      "source": [
        "for trigrama in trigramas:\n",
        "  if trigrama[0][0].isupper() and trigrama[1][0].isupper() and trigrama[2][0].isupper():\n",
        "    print(trigrama)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2JMmhAo0c01"
      },
      "source": [
        "# Stemming e Lematização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PlCFPNW0axW"
      },
      "source": [
        "import nltk\n",
        "\n",
        "stemmer = nltk.RSLPStemmer()\n",
        "\n",
        "print(stemmer.stem(\"Amigão\"))\n",
        "print(stemmer.stem(\"amigo\"))\n",
        "print(stemmer.stem(\"amigos\"))\n",
        "print(stemmer.stem(\"propuseram\"))\n",
        "print(stemmer.stem(\"propõem\"))\n",
        "print(stemmer.stem(\"propondo\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eSdI58B2JIl"
      },
      "source": [
        "# Etiquetador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPTMvQrR1PHM"
      },
      "source": [
        "from nltk.corpus import mac_morpho\n",
        "from nltk.tag import UnigramTagger\n",
        "\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "\n",
        "sentencas_treino = mac_morpho.tagged_sents()\n",
        "etiquetador = UnigramTagger(sentencas_treino)\n",
        "\n",
        "etiquetado = etiquetador.tag(tokens)\n",
        "\n",
        "print(etiquetado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk_5pOAP4McN"
      },
      "source": [
        "from nltk.corpus import mac_morpho\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.tag import DefaultTagger\n",
        "\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "\n",
        "# Dessa vez utilizaremos o DefaultTagger para definir uma etiqueta padrão\n",
        "etiq_padrao = DefaultTagger('N')\n",
        "sentencas_treino = mac_morpho.tagged_sents()\n",
        "etiquetador = UnigramTagger(sentencas_treino, backoff=etiq_padrao)\n",
        "\n",
        "etiquetado = etiquetador.tag(tokens)\n",
        "\n",
        "etiquetado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH4_nHDI5Wo9"
      },
      "source": [
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "pattern = 'NP: {<NPROP><NPROP> | <N><N>}'\n",
        "analise_gramatical = RegexpParser(pattern)\n",
        "\n",
        "arvore = analise_gramatical.parse(etiquetado)\n",
        "print(arvore)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}